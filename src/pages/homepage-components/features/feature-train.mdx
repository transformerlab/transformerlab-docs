## Pre-training, Finetuning, RLHF and Preference Optimization

Build advanced models from scratch or fine-tune existing ones with production-ready implementations of DPO, ORPO, SIMPO, and GRPO that work out of the box. No more wrestling with framework incompatibilities or debugging distributed training setups. Complete RLHF pipeline with reward modeling handles the complex orchestration automatically, from data processing to final model outputs.